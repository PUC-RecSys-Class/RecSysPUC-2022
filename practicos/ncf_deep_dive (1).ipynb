{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-BC_4dycuTo"
      },
      "source": [
        "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
        "\n",
        "<i>Licensed under the MIT License.</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_0OjAYpcuTq"
      },
      "source": [
        "# Neural Collaborative Filtering (NCF)\n",
        "\n",
        "This notebook serves as an introduction to Neural Collaborative Filtering (NCF), which is an innovative algorithm based on deep neural networks to tackle the key problem in recommendation — collaborative filtering — on the basis of implicit feedback.\n",
        "\n",
        "Paper: https://arxiv.org/pdf/1708.05031.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL2rStHrcz3V",
        "outputId": "d8ae8bee-aacb-4afe-dbcf-7f799ea09262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting recommenders[examples,gpu]\n",
            "  Downloading recommenders-1.1.1-py3-none-any.whl (339 kB)\n",
            "\u001b[K     |████████████████████████████████| 339 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting category-encoders<2,>=1.3.0\n",
            "  Downloading category_encoders-1.3.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (1.21.6)\n",
            "Collecting memory-profiler<1,>=0.54.0\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Requirement already satisfied: matplotlib<4,>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (3.2.2)\n",
            "Collecting lightfm<2,>=1.15\n",
            "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
            "\u001b[K     |████████████████████████████████| 310 kB 50.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (2.23.0)\n",
            "Collecting transformers<5,>=2.5.0\n",
            "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 50.1 MB/s \n",
            "\u001b[?25hCollecting retrying>=1.3.3\n",
            "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
            "Collecting pandera[strategies]>=0.6.5\n",
            "  Downloading pandera-0.9.0-py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 14.3 MB/s \n",
            "\u001b[?25hCollecting cornac<2,>=1.1.2\n",
            "  Downloading cornac-1.14.2-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 34.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk<4,>=3.4 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (3.7)\n",
            "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (1.7.3)\n",
            "Collecting pyyaml<6,>=5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 56.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba<1,>=0.38.1 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (0.56.2)\n",
            "Requirement already satisfied: seaborn<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (0.11.2)\n",
            "Requirement already satisfied: pandas<2,>1.0.3 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (1.3.5)\n",
            "Requirement already satisfied: lightgbm>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (2.2.3)\n",
            "Requirement already satisfied: tqdm<5,>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (4.64.1)\n",
            "Requirement already satisfied: jinja2<3.1,>=2 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (2.11.3)\n",
            "Collecting scikit-surprise>=1.0.6\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 16.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<1.0.3,>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (1.0.2)\n",
            "Collecting bottleneck<2,>=1.2.1\n",
            "  Downloading Bottleneck-1.3.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
            "\u001b[K     |████████████████████████████████| 355 kB 61.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipykernel<7,>=4.6.1 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (5.3.4)\n",
            "Collecting jupyter<2,>=1\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Collecting azure.mgmt.cosmosdb<1,>=0.8.0\n",
            "  Downloading azure_mgmt_cosmosdb-0.16.0-py2.py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 45.6 MB/s \n",
            "\u001b[?25hCollecting scrapbook<1.0.0,>=0.5.0\n",
            "  Downloading scrapbook-0.5.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: hyperopt<1,>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (0.1.2)\n",
            "Collecting papermill<3,>=2.1.2\n",
            "  Downloading papermill-2.4.0-py3-none-any.whl (38 kB)\n",
            "Collecting locust<2,>=1\n",
            "  Downloading locust-1.6.0-py3-none-any.whl (766 kB)\n",
            "\u001b[K     |████████████████████████████████| 766 kB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.7/dist-packages (from recommenders[examples,gpu]) (1.12.1+cu113)\n",
            "Collecting nvidia-ml-py3>=7.352.0\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "Collecting tensorflow~=2.7.0\n",
            "  Downloading tensorflow-2.7.4-cp37-cp37m-manylinux2010_x86_64.whl (495.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 495.5 MB 10 kB/s \n",
            "\u001b[?25hCollecting tf-slim>=1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 77.1 MB/s \n",
            "\u001b[?25hCollecting fastai<2,>=1.0.46\n",
            "  Downloading fastai-1.0.61-py3-none-any.whl (239 kB)\n",
            "\u001b[K     |████████████████████████████████| 239 kB 62.2 MB/s \n",
            "\u001b[?25hCollecting msrest>=0.5.0\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting azure-common~=1.1\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Collecting msrestazure<2.0.0,>=0.4.32\n",
            "  Downloading msrestazure-0.6.4-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders[examples,gpu]) (0.12.2)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders[examples,gpu]) (0.5.2)\n",
            "Collecting powerlaw\n",
            "  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from fastai<2,>=1.0.46->recommenders[examples,gpu]) (0.13.1+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai<2,>=1.0.46->recommenders[examples,gpu]) (21.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fastai<2,>=1.0.46->recommenders[examples,gpu]) (7.1.2)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.7/dist-packages (from fastai<2,>=1.0.46->recommenders[examples,gpu]) (3.4.1)\n",
            "Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from fastai<2,>=1.0.46->recommenders[examples,gpu]) (1.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from fastai<2,>=1.0.46->recommenders[examples,gpu]) (4.6.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from fastai<2,>=1.0.46->recommenders[examples,gpu]) (2.8.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt<1,>=0.1.2->recommenders[examples,gpu]) (2.6.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt<1,>=0.1.2->recommenders[examples,gpu]) (4.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt<1,>=0.1.2->recommenders[examples,gpu]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt<1,>=0.1.2->recommenders[examples,gpu]) (1.15.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (6.1.12)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (7.9.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (5.1.1)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (0.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (2.0.10)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (0.7.5)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3.1,>=2->recommenders[examples,gpu]) (2.0.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter<2,>=1->recommenders[examples,gpu]) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter<2,>=1->recommenders[examples,gpu]) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter<2,>=1->recommenders[examples,gpu]) (5.3.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter<2,>=1->recommenders[examples,gpu]) (7.7.1)\n",
            "Collecting qtconsole\n",
            "  Downloading qtconsole-5.3.2-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 60.4 MB/s \n",
            "\u001b[?25hCollecting flask==1.1.2\n",
            "  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting Flask-Cors>=3.0.10\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: msgpack>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from locust<2,>=1->recommenders[examples,gpu]) (1.0.4)\n",
            "Collecting gevent>=20.9.0\n",
            "  Downloading gevent-21.12.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyzmq>=16.0.2 in /usr/local/lib/python3.7/dist-packages (from locust<2,>=1->recommenders[examples,gpu]) (23.2.1)\n",
            "Requirement already satisfied: Werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from locust<2,>=1->recommenders[examples,gpu]) (1.0.1)\n",
            "Collecting psutil>=5.6.7\n",
            "  Downloading psutil-5.9.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[K     |████████████████████████████████| 281 kB 75.3 MB/s \n",
            "\u001b[?25hCollecting ConfigArgParse>=1.0\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Collecting geventhttpclient>=1.4.4\n",
            "  Downloading geventhttpclient-2.0.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 9.7 MB/s \n",
            "\u001b[?25hCollecting Flask-BasicAuth>=0.2.0\n",
            "  Downloading Flask-BasicAuth-0.2.0.tar.gz (16 kB)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->locust<2,>=1->recommenders[examples,gpu]) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->locust<2,>=1->recommenders[examples,gpu]) (7.1.2)\n",
            "Collecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent>=20.9.0->locust<2,>=1->recommenders[examples,gpu]) (1.1.3)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 73.1 MB/s \n",
            "\u001b[?25hCollecting brotli\n",
            "  Downloading Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[K     |████████████████████████████████| 357 kB 50.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from geventhttpclient>=1.4.4->locust<2,>=1->recommenders[examples,gpu]) (2022.6.15)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders[examples,gpu]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders[examples,gpu]) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders[examples,gpu]) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders[examples,gpu]) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4,>=2.2.2->recommenders[examples,gpu]) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.5.0->azure.mgmt.cosmosdb<1,>=0.8.0->recommenders[examples,gpu]) (1.3.1)\n",
            "Collecting isodate>=0.6.0\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 609 kB/s \n",
            "\u001b[?25hCollecting azure-core>=1.24.0\n",
            "  Downloading azure_core-1.25.1-py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 61.6 MB/s \n",
            "\u001b[?25hCollecting adal<2.0.0,>=0.6.0\n",
            "  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting PyJWT<3,>=1.0.0\n",
            "  Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB)\n",
            "Collecting cryptography>=1.1.0\n",
            "  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.1.0->adal<2.0.0,>=0.6.0->msrestazure<2.0.0,>=0.4.32->azure.mgmt.cosmosdb<1,>=0.8.0->recommenders[examples,gpu]) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.1.0->adal<2.0.0,>=0.6.0->msrestazure<2.0.0,>=0.4.32->azure.mgmt.cosmosdb<1,>=0.8.0->recommenders[examples,gpu]) (2.21)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders[examples,gpu]) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders[examples,gpu]) (1.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders[examples,gpu]) (4.12.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders[examples,gpu]) (0.39.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>1.0.3->recommenders[examples,gpu]) (2022.2.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders[examples,gpu]) (1.14.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders[examples,gpu]) (6.0.1)\n",
            "Collecting typing-inspect>=0.6.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders[examples,gpu]) (1.9.2)\n",
            "Collecting hypothesis>=5.41.1\n",
            "  Downloading hypothesis-6.54.6-py3-none-any.whl (390 kB)\n",
            "\u001b[K     |████████████████████████████████| 390 kB 62.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders[examples,gpu]) (22.1.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders[examples,gpu]) (2.4.0)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.0.0rc9-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.7/dist-packages (from papermill<3,>=2.1.2->recommenders[examples,gpu]) (8.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from papermill<3,>=2.1.2->recommenders[examples,gpu]) (0.4)\n",
            "Collecting nbclient>=0.2.0\n",
            "  Downloading nbclient-0.6.8-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 192 kB/s \n",
            "\u001b[?25hCollecting ansiwrap\n",
            "  Downloading ansiwrap-0.8.4-py2.py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from papermill<3,>=2.1.2->recommenders[examples,gpu]) (5.4.0)\n",
            "Collecting nest-asyncio\n",
            "  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n",
            "Collecting traitlets>=4.1.0\n",
            "  Downloading traitlets-5.4.0-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (4.11.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill<3,>=2.1.2->recommenders[examples,gpu]) (2.16.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill<3,>=2.1.2->recommenders[examples,gpu]) (4.3.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill<3,>=2.1.2->recommenders[examples,gpu]) (5.9.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill<3,>=2.1.2->recommenders[examples,gpu]) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=5.1.2->papermill<3,>=2.1.2->recommenders[examples,gpu]) (3.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel<7,>=4.6.1->recommenders[examples,gpu]) (0.2.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders[examples,gpu]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders[examples,gpu]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders[examples,gpu]) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure.mgmt.cosmosdb<1,>=0.8.0->recommenders[examples,gpu]) (3.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.0.3,>=0.22.1->recommenders[examples,gpu]) (3.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (0.6.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (3.0.10)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (1.0.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (8.1.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (0.4.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (1.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (2.4.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (5.2.1)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (2.8.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (1.48.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (1.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (14.0.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (3.17.3)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (2.0.7)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (0.26.0)\n",
            "Collecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (0.37.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->recommenders[examples,gpu]) (1.1.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.7.0->recommenders[examples,gpu]) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->recommenders[examples,gpu]) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->recommenders[examples,gpu]) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->recommenders[examples,gpu]) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->recommenders[examples,gpu]) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->recommenders[examples,gpu]) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->recommenders[examples,gpu]) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->recommenders[examples,gpu]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->recommenders[examples,gpu]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->recommenders[examples,gpu]) (0.4.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.18->fastai<2,>=1.0.46->recommenders[examples,gpu]) (0.7.8)\n",
            "Collecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 62.7 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders[examples,gpu]) (3.8.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Collecting textwrap3>=0.9.2\n",
            "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter<2,>=1->recommenders[examples,gpu]) (3.0.3)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter<2,>=1->recommenders[examples,gpu]) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter<2,>=1->recommenders[examples,gpu]) (3.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter<2,>=1->recommenders[examples,gpu]) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter<2,>=1->recommenders[examples,gpu]) (0.13.3)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter<2,>=1->recommenders[examples,gpu]) (0.7.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->recommenders[examples,gpu]) (5.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->recommenders[examples,gpu]) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->recommenders[examples,gpu]) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->recommenders[examples,gpu]) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->recommenders[examples,gpu]) (1.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter<2,>=1->recommenders[examples,gpu]) (0.5.1)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from powerlaw->cornac<2,>=1.1.2->recommenders[examples,gpu]) (1.2.1)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.2.0-py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 557 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: lightfm, Flask-BasicAuth, memory-profiler, nvidia-ml-py3, retrying, scikit-surprise\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=705384 sha256=1625ba603e1d8aa9a537e5b362ea7927ee2322bc2274412c98dab353a5476c6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/56/28/5772a3bd3413d65f03aa452190b00898b680b10028a1021914\n",
            "  Building wheel for Flask-BasicAuth (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Flask-BasicAuth: filename=Flask_BasicAuth-0.2.0-py3-none-any.whl size=4243 sha256=3555e3657bc57227232cf706ec352e8f7934dc667afe17674eeee797c431ec74\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/08/a3/19638d90fdf01258ede772449bcbde424839459749acb977b6\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=cc4410bb0d2520d88eef06947e290f22d8a65eb30d7cd825d5ac45a4907cfdd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19190 sha256=964d38d507dd0c6b5ae64db46b218a25bef9e20e86bda00ee4b2f3da2c10eaf1\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/99/da/c34f202dc8fd1dffd35e0ecf1a7d7f8374ca05fbcbaf974b83\n",
            "  Building wheel for retrying (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11447 sha256=dee607943cca4d1c425d00ff64a51ead0e978cd6fbe57eada02dd012186f1714\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/8d/8d/f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1633991 sha256=e1830e3b100286024e511a32baabd38b8e31ac2623db58b3c73e40838e6fd53c\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built lightfm Flask-BasicAuth memory-profiler nvidia-ml-py3 retrying scikit-surprise\n",
            "Installing collected packages: traitlets, jedi, mypy-extensions, zope.interface, zope.event, typing-inspect, textwrap3, pyyaml, PyJWT, nest-asyncio, isodate, exceptiongroup, cryptography, azure-core, tokenizers, qtpy, psutil, powerlaw, pandera, nbclient, msrest, hypothesis, huggingface-hub, gevent, flask, brotli, ansiwrap, adal, transformers, tensorflow-estimator, scikit-surprise, retrying, qtconsole, papermill, nvidia-ml-py3, msrestazure, memory-profiler, lightfm, keras, geventhttpclient, gast, Flask-Cors, Flask-BasicAuth, cornac, ConfigArgParse, category-encoders, bottleneck, azure-common, tf-slim, tensorflow, scrapbook, recommenders, locust, jupyter, fastai, azure.mgmt.cosmosdb\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 2.7.9\n",
            "    Uninstalling fastai-2.7.9:\n",
            "      Successfully uninstalled fastai-2.7.9\n",
            "Successfully installed ConfigArgParse-1.5.3 Flask-BasicAuth-0.2.0 Flask-Cors-3.0.10 PyJWT-2.5.0 adal-1.2.7 ansiwrap-0.8.4 azure-common-1.1.28 azure-core-1.25.1 azure.mgmt.cosmosdb-0.16.0 bottleneck-1.3.5 brotli-1.0.9 category-encoders-1.3.0 cornac-1.14.2 cryptography-38.0.1 exceptiongroup-1.0.0rc9 fastai-1.0.61 flask-1.1.2 gast-0.4.0 gevent-21.12.0 geventhttpclient-2.0.2 huggingface-hub-0.9.1 hypothesis-6.54.6 isodate-0.6.1 jedi-0.18.1 jupyter-1.0.0 keras-2.7.0 lightfm-1.16 locust-1.6.0 memory-profiler-0.60.0 msrest-0.7.1 msrestazure-0.6.4 mypy-extensions-0.4.3 nbclient-0.6.8 nest-asyncio-1.5.5 nvidia-ml-py3-7.352.0 pandera-0.9.0 papermill-2.4.0 powerlaw-1.5 psutil-5.9.2 pyyaml-5.4.1 qtconsole-5.3.2 qtpy-2.2.0 recommenders-1.1.1 retrying-1.3.3 scikit-surprise-1.1.1 scrapbook-0.5.0 tensorflow-2.7.4 tensorflow-estimator-2.7.0 textwrap3-0.9.2 tf-slim-1.1.0 tokenizers-0.12.1 traitlets-5.4.0 transformers-4.22.1 typing-inspect-0.8.0 zope.event-4.5.0 zope.interface-5.4.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install recommenders[examples,gpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhHcmZRQddgR",
        "outputId": "e4d3d97f-b7f0-463d-ef0d-740917db828b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf_slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (1.2.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tf_slim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGmRuGU2cuTq"
      },
      "source": [
        "## 0 Global Settings and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AX37ZJ3JcuTr",
        "outputId": "44d76ac7-11b7-42ae-ad42-dfeef2e33ba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System version: 3.7.14 (default, Sep  8 2022, 00:06:44) \n",
            "[GCC 7.5.0]\n",
            "Pandas version: 1.3.5\n",
            "Tensorflow version: 2.8.2\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import papermill as pm\n",
        "import scrapbook as sb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.models.ncf.ncf_singlenode import NCF\n",
        "from recommenders.models.ncf.dataset import Dataset as NCFDataset\n",
        "from recommenders.datasets import movielens\n",
        "from recommenders.datasets.python_splitters import python_chrono_split\n",
        "from recommenders.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k, \n",
        "                                                     recall_at_k, get_top_k_items)\n",
        "from recommenders.utils.constants import SEED as DEFAULT_SEED\n",
        "\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Pandas version: {}\".format(pd.__version__))\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djBxfQ5ocuTs"
      },
      "outputs": [],
      "source": [
        "# top k items to recommend\n",
        "TOP_K = 10\n",
        "\n",
        "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
        "MOVIELENS_DATA_SIZE = '100k'\n",
        "\n",
        "# Model parameters\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "SEED = DEFAULT_SEED  # Set None for non-deterministic results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBhC6IEkcuTs"
      },
      "source": [
        "## 1 Matrix factorization algorithm\n",
        "\n",
        "NCF is a neural matrix factorization model, which ensembles Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP) to unify the strengths of linearity of MF and non-linearity of MLP for modelling the user–item latent structures. NCF can be demonstrated as a framework for GMF and MLP, which is illustrated as below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gtubJw_cuTt"
      },
      "source": [
        "<img src=\"https://recodatasets.z20.web.core.windows.net/images/NCF.svg?sanitize=true\">\n",
        "\n",
        "This figure shows how to utilize latent vectors of items and users, and then how to fuse outputs from GMF Layer (left) and MLP Layer (right). We will introduce this framework and show how to learn the model parameters in following sections.\n",
        "\n",
        "### 1.1 The GMF model\n",
        "\n",
        "In ALS, the ratings are modeled as follows:\n",
        "\n",
        "$$\\hat { r } _ { u , i } = q _ { i } ^ { T } p _ { u }$$\n",
        "\n",
        "GMF introduces a neural CF layer as the output layer of standard MF. In this way, MF can be easily generalized\n",
        "and extended. For example, if we allow the edge weights of this output layer to be learnt from data without the uniform constraint, it will result in a variant of MF that allows varying importance of latent dimensions. And if we use a non-linear function for activation, it will generalize MF to a non-linear setting which might be more expressive than the linear MF model. GMF can be shown as follows:\n",
        "\n",
        "$$\\hat { r } _ { u , i } = a _ { o u t } \\left( h ^ { T } \\left( q _ { i } \\odot p _ { u } \\right) \\right)$$\n",
        "\n",
        "where $\\odot$ is element-wise product of vectors. Additionally, ${a}_{out}$ and ${h}$ denote the activation function and edge weights of the output layer respectively. MF can be interpreted as a special case of GMF. Intuitively, if we use an identity function for ${a}_{out}$ and enforce ${h}$ to be a uniform vector of 1, we can exactly recover the MF model.\n",
        "\n",
        "### 1.2 The MLP model\n",
        "\n",
        "NCF adopts two pathways to model users and items: 1) element-wise product of vectors, 2) concatenation of vectors. To learn interactions after concatenating of users and items latent features, the standard MLP model is applied. In this sense, we can endow the model a large level of flexibility and non-linearity to learn the interactions between $p_{u}$ and $q_{i}$. The details of MLP model are:\n",
        "\n",
        "For the input layer, there is concatenation of user and item vectors:\n",
        "\n",
        "$$z _ { 1 } = \\phi _ { 1 } \\left( p _ { u } , q _ { i } \\right) = \\left[ \\begin{array} { c } { p _ { u } } \\\\ { q _ { i } } \\end{array} \\right]$$\n",
        "\n",
        "So for the hidden layers and output layer of MLP, the details are:\n",
        "\n",
        "$$\n",
        "\\phi _ { l } \\left( z _ { l } \\right) = a _ { o u t } \\left( W _ { l } ^ { T } z _ { l } + b _ { l } \\right) , ( l = 2,3 , \\ldots , L - 1 )\n",
        "$$\n",
        "\n",
        "and:\n",
        "\n",
        "$$\n",
        "\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\phi \\left( z _ { L - 1 } \\right) \\right)\n",
        "$$\n",
        "\n",
        "where ${ W }_{ l }$, ${ b }_{ l }$, and ${ a }_{ out }$ denote the weight matrix, bias vector, and activation function for the $l$-th layer’s perceptron, respectively. For activation functions of MLP layers, one can freely choose sigmoid, hyperbolic tangent (tanh), and Rectifier (ReLU), among others. Because we have a binary classification task, the activation function of the output layer is defined as sigmoid $\\sigma(x)=\\frac{1}{1+e^{-x}}$ to restrict the predicted score to be in (0,1).\n",
        "\n",
        "\n",
        "### 1.3 Fusion of GMF and MLP\n",
        "\n",
        "To provide more flexibility to the fused model, we allow GMF and MLP to learn separate embeddings, and combine the two models by concatenating their last hidden layer. We get $\\phi^{GMF}$ from GMF:\n",
        "\n",
        "$$\\phi _ { u , i } ^ { G M F } = p _ { u } ^ { G M F } \\odot q _ { i } ^ { G M F }$$\n",
        "\n",
        "and obtain $\\phi^{MLP}$ from MLP:\n",
        "\n",
        "$$\\phi _ { u , i } ^ { M L P } = a _ { o u t } \\left( W _ { L } ^ { T } \\left( a _ { o u t } \\left( \\ldots a _ { o u t } \\left( W _ { 2 } ^ { T } \\left[ \\begin{array} { c } { p _ { u } ^ { M L P } } \\\\ { q _ { i } ^ { M L P } } \\end{array} \\right] + b _ { 2 } \\right) \\ldots \\right) \\right) + b _ { L }\\right.$$\n",
        "\n",
        "Lastly, we fuse output from GMF and MLP:\n",
        "\n",
        "$$\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\left[ \\begin{array} { l } { \\phi ^ { G M F } } \\\\ { \\phi ^ { M L P } } \\end{array} \\right] \\right)$$\n",
        "\n",
        "This model combines the linearity of MF and non-linearity of DNNs for modelling user–item latent structures.\n",
        "\n",
        "### 1.4 Objective Function\n",
        "\n",
        "We define the likelihood function as:\n",
        "\n",
        "$$P \\left( \\mathcal { R } , \\mathcal { R } ^ { - } | \\mathbf { P } , \\mathbf { Q } , \\Theta \\right) = \\prod _ { ( u , i ) \\in \\mathcal { R } } \\hat { r } _ { u , i } \\prod _ { ( u , j ) \\in \\mathcal { R } ^{ - } } \\left( 1 - \\hat { r } _ { u , j } \\right)$$\n",
        "\n",
        "Where $\\mathcal{R}$ denotes the set of observed interactions, and $\\mathcal{ R } ^ { - }$ denotes the set of negative instances. $\\mathbf{P}$ and $\\mathbf{Q}$ denotes the latent factor matrix for users and items, respectively; and $\\Theta$ denotes the model parameters. Taking the negative logarithm of the likelihood, we obtain the objective function to minimize for NCF method, which is known as [binary cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy):\n",
        "\n",
        "$$L = - \\sum _ { ( u , i ) \\in \\mathcal { R } \\cup { \\mathcal { R } } ^ { - } } r _ { u , i } \\log \\hat { r } _ { u , i } + \\left( 1 - r _ { u , i } \\right) \\log \\left( 1 - \\hat { r } _ { u , i } \\right)$$\n",
        "\n",
        "The optimization can be done by performing Stochastic Gradient Descent (SGD), which is described in the [Surprise SVD deep dive notebook](surprise_svd_deep_dive.ipynb). Our SGD method is very similar to the SVD algorithm's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm4zTuKWcuTu"
      },
      "source": [
        "## 2 TensorFlow implementation of NCF\n",
        "\n",
        "We will use the MovieLens dataset, which is composed of integer ratings from 1 to 5.\n",
        "\n",
        "We convert MovieLens into implicit feedback, and evaluate under our *leave-one-out* evaluation protocol.\n",
        "\n",
        "You can check the details of implementation in `recommenders/models/ncf`\n",
        "Link: https://github.com/microsoft/recommenders/blob/main/recommenders/models/ncf/ncf_singlenode.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vamwk8M8cuTv"
      },
      "source": [
        "## 3 TensorFlow NCF movie recommender\n",
        "\n",
        "### 3.1 Load and split data\n",
        "\n",
        "To evaluate the performance of item recommendation, we adopt the leave-one-out evaluation.\n",
        "\n",
        "For each user, we held out his/her last interaction as the test set and utilized the remaining data for training. Since it is too time-consuming to rank all items for every user during evaluation, we followed the common strategy that randomly samples 100 items that are not interacted by the user, ranking the test item among the 100 items. Our test samples will be constructed by `NCFDataset`.\n",
        "\n",
        "We also show an alternative evaluation method, splitting the data chronologically using `python_chrono_split` to achieve a 75/25% training and test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "g_fgr3XTcuTw",
        "outputId": "d313f35d-f7d3-4fef-dd2a-4255b7db52d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.81k/4.81k [00:01<00:00, 2.67kKB/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7d6d0aee-71f5-4794-9000-bfa43962dc31\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>196</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>881250949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>186</td>\n",
              "      <td>302</td>\n",
              "      <td>3.0</td>\n",
              "      <td>891717742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>377</td>\n",
              "      <td>1.0</td>\n",
              "      <td>878887116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>244</td>\n",
              "      <td>51</td>\n",
              "      <td>2.0</td>\n",
              "      <td>880606923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>166</td>\n",
              "      <td>346</td>\n",
              "      <td>1.0</td>\n",
              "      <td>886397596</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d6d0aee-71f5-4794-9000-bfa43962dc31')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7d6d0aee-71f5-4794-9000-bfa43962dc31 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7d6d0aee-71f5-4794-9000-bfa43962dc31');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   userID  itemID  rating  timestamp\n",
              "0     196     242     3.0  881250949\n",
              "1     186     302     3.0  891717742\n",
              "2      22     377     1.0  878887116\n",
              "3     244      51     2.0  880606923\n",
              "4     166     346     1.0  886397596"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = movielens.load_pandas_df(\n",
        "    size=MOVIELENS_DATA_SIZE,\n",
        "    header=[\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n",
        ")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0az7MdBxcuTw"
      },
      "outputs": [],
      "source": [
        "train, test = python_chrono_split(df, 0.75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR3d-Wl6cuTx"
      },
      "source": [
        "Filter out any users or items in the test set that do not appear in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SF3w5DmcuTx"
      },
      "outputs": [],
      "source": [
        "test = test[test[\"userID\"].isin(train[\"userID\"].unique())]\n",
        "test = test[test[\"itemID\"].isin(train[\"itemID\"].unique())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uBWLSBNcuTx"
      },
      "source": [
        "Create a test set containing the last interaction for each user as for the leave-one-out evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugEqZUjvcuTy"
      },
      "outputs": [],
      "source": [
        "leave_one_out_test = test.groupby(\"userID\").last().reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfD3SJagcuTy"
      },
      "source": [
        "Write datasets to csv files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhAFAOgbcuTy"
      },
      "outputs": [],
      "source": [
        "train_file = \"./train.csv\"\n",
        "test_file = \"./test.csv\"\n",
        "leave_one_out_test_file = \"./leave_one_out_test.csv\"\n",
        "train.to_csv(train_file, index=False)\n",
        "test.to_csv(test_file, index=False)\n",
        "leave_one_out_test.to_csv(leave_one_out_test_file, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNAJWYWOcuTy"
      },
      "source": [
        "### 3.2 Functions of NCF Dataset \n",
        "\n",
        "Important functions of the Dataset class for NCF:\n",
        "\n",
        "`train_loader(batch_size, shuffle_size)`, generate training batches of size `batch_size`. Positive examples are loaded from the training file and negative samples are added in memory. `shuffle_size` determines the number of rows that are read into memory before the examples are shuffled. By default, the function will attempt to load all data before performing the shuffle. If memory constraints are encountered when using large datasets, try reducing `shuffle_size`.\n",
        "\n",
        "`test_loader()`, generate test batch by every positive test instance, (eg. `[1, 2, 1]` is a positive user & item pair in test set (`[userID, itemID, rating]` for this tuple). This function returns data like `[[1, 2, 1], [1, 3, 0], [1,6, 0], ...]`, ie. following our *leave-one-out* evaluation protocol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kvUKCRmcuTy",
        "outputId": "5065eaeb-af34-4692-bd47-ef620a69f3bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 943/943 [00:05<00:00, 157.19it/s]\n"
          ]
        }
      ],
      "source": [
        "data = NCFDataset(train_file=train_file, test_file=leave_one_out_test_file, seed=SEED, overwrite_test_file_full=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQciqV5PcuTz"
      },
      "source": [
        "### 3.3 Train NCF based on TensorFlow\n",
        "The NCF has a lot of parameters. The most important ones are:\n",
        "\n",
        "`n_factors`, which controls the dimension of the latent space. Usually, the quality of the training set predictions grows with as n_factors gets higher.\n",
        "\n",
        "`layer_sizes`, sizes of input layer (and hidden layers) of MLP, input type is list.\n",
        "\n",
        "`n_epochs`, which defines the number of iteration of the SGD procedure.\n",
        "Note that both parameter also affect the training time.\n",
        "\n",
        "`model_type`, we can train single `\"MLP\"`, `\"GMF\"` or combined model `\"NCF\"` by changing the type of model.\n",
        "\n",
        "We will here set `n_factors` to 4, `layer_sizes` to `[16,8,4]`,  `n_epochs` to 100, `batch_size` to 256. To train the model, we simply need to call the `fit()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWZ-JNTIcuTz",
        "outputId": "83579699-d7dc-431f-8785-3305b94cf7c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        }
      ],
      "source": [
        "model = NCF (\n",
        "    n_users=data.n_users, \n",
        "    n_items=data.n_items,\n",
        "    model_type=\"NeuMF\",\n",
        "    n_factors=4,\n",
        "    layer_sizes=[16,8,4],\n",
        "    n_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=1e-3,\n",
        "    verbose=10,\n",
        "    seed=SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ItTkLpzUcuTz",
        "outputId": "f35b445f-f236-4381-89e1-885669e8fca6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Took 842.3135722389999 seconds for training.\n"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    model.fit(data)\n",
        "\n",
        "print(\"Took {} seconds for training.\".format(train_time.interval))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZo5UvYtcuT0"
      },
      "source": [
        "## 3.4 Prediction and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w1mRmUIcuT0"
      },
      "source": [
        "### 3.4.1 Prediction\n",
        "\n",
        "Now that our model is fitted, we can call `predict` to get some `predictions`. `predict` returns an internal object Prediction which can be easily converted back to a dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wy1FykQNcuT0",
        "outputId": "66a46265-7b0e-4274-92ae-ce0db0f965cb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-30426bae-9f8a-45db-ab94-c01f1fb2116b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>149.0</td>\n",
              "      <td>0.027429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>0.462700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>0.202064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>0.076311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>0.199332</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30426bae-9f8a-45db-ab94-c01f1fb2116b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-30426bae-9f8a-45db-ab94-c01f1fb2116b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-30426bae-9f8a-45db-ab94-c01f1fb2116b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   userID  itemID  prediction\n",
              "0     1.0   149.0    0.027429\n",
              "1     1.0    88.0    0.462700\n",
              "2     1.0   101.0    0.202064\n",
              "3     1.0   110.0    0.076311\n",
              "4     1.0   103.0    0.199332"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = [[row.userID, row.itemID, model.predict(row.userID, row.itemID)]\n",
        "               for (_, row) in test.iterrows()]\n",
        "\n",
        "\n",
        "predictions = pd.DataFrame(predictions, columns=['userID', 'itemID', 'prediction'])\n",
        "predictions.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K060ogbvcuT1"
      },
      "source": [
        "### 3.4.2 Generic Evaluation\n",
        "We remove rated movies in the top k recommendations\n",
        "To compute ranking metrics, we need predictions on all user, item pairs. We remove though the items already watched by the user, since we choose not to recommend them again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bZSvHGbecuT1",
        "outputId": "84c909f7-423b-4b55-ea96-6377336f9e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Took 4.3498592519999875 seconds for prediction.\n"
          ]
        }
      ],
      "source": [
        "with Timer() as test_time:\n",
        "\n",
        "    users, items, preds = [], [], []\n",
        "    item = list(train.itemID.unique())\n",
        "    for user in train.userID.unique():\n",
        "        user = [user] * len(item) \n",
        "        users.extend(user)\n",
        "        items.extend(item)\n",
        "        preds.extend(list(model.predict(user, item, is_list=True)))\n",
        "\n",
        "    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
        "\n",
        "    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
        "    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
        "\n",
        "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SB51RU_HcuT1",
        "outputId": "948b7bbb-ce4d-422c-f64f-1d96ce704d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAP:\t0.048158\n",
            "NDCG:\t0.198736\n",
            "Precision@K:\t0.179958\n",
            "Recall@K:\t0.100628\n"
          ]
        }
      ],
      "source": [
        "\n",
        "eval_map = map_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "eval_ndcg = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "eval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "eval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "\n",
        "print(\"MAP:\\t%f\" % eval_map,\n",
        "      \"NDCG:\\t%f\" % eval_ndcg,\n",
        "      \"Precision@K:\\t%f\" % eval_precision,\n",
        "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB4XxpEqcuT2"
      },
      "source": [
        "### 3.4.3 \"Leave-one-out\" Evaluation\n",
        "\n",
        "We implement the functions to repoduce the leave-one-out evaluation protocol mentioned in original NCF paper.\n",
        "\n",
        "For each item in test data, we randomly samples 100 items that are not interacted by the user, ranking the test item among the 101 items (1 positive item and 100 negative items). The performance of a ranked list is judged by **Hit Ratio (HR)** and **Normalized Discounted Cumulative Gain (NDCG)**. Finally, we average the values of those ranked lists to obtain the overall HR and NDCG on test data.\n",
        "\n",
        "We truncated the ranked list at 10 for both metrics. As such, the HR intuitively measures whether the test item is present on the top-10 list, and the NDCG accounts for the position of the hit by assigning higher scores to hits at top ranks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zb_zZSDEcuT2",
        "outputId": "9e4fed28-0fb0-4bc8-af4a-83cb9f7862f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HR:\t0.517497\n",
            "NDCG:\t0.399773\n"
          ]
        }
      ],
      "source": [
        "k = TOP_K\n",
        "\n",
        "ndcgs = []\n",
        "hit_ratio = []\n",
        "\n",
        "for b in data.test_loader():\n",
        "    user_input, item_input, labels = b\n",
        "    output = model.predict(user_input, item_input, is_list=True)\n",
        "\n",
        "    output = np.squeeze(output)\n",
        "    rank = sum(output >= output[0])\n",
        "    if rank <= k:\n",
        "        ndcgs.append(1 / np.log(rank + 1))\n",
        "        hit_ratio.append(1)\n",
        "    else:\n",
        "        ndcgs.append(0)\n",
        "        hit_ratio.append(0)\n",
        "\n",
        "eval_ndcg = np.mean(ndcgs)\n",
        "eval_hr = np.mean(hit_ratio)\n",
        "\n",
        "print(\"HR:\\t%f\" % eval_hr)\n",
        "print(\"NDCG:\\t%f\" % eval_ndcg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKsJVob6cuT2"
      },
      "source": [
        "## 3.5 Pre-training (Actividad)\n",
        "\n",
        "La actividad consiste en básicamente que entrenen NeuMF preentrenandolo primero. Basta con que corran las siguientes celdas y al final comenten acerca de sus resultados.\n",
        "\n",
        "To get better performance of NeuMF, we can adopt pre-training strategy. We first train GMF and MLP with random initializations until convergence. Then use their model parameters as the initialization for the corresponding parts of NeuMF’s parameters.  Please pay attention to the output layer, where we concatenate weights of the two models with\n",
        "\n",
        "$$h ^ { N C F } \\leftarrow \\left[ \\begin{array} { c } { \\alpha h ^ { G M F } } \\\\ { ( 1 - \\alpha ) h ^ { M L P } } \\end{array} \\right]$$\n",
        "\n",
        "where $h^{GMF}$ and $h^{MLP}$ denote the $h$ vector of the pretrained GMF and MLP model, respectively; and $\\alpha$ is a\n",
        "hyper-parameter determining the trade-off between the two pre-trained models. We set $\\alpha$ = 0.5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mr2jxMFcuT2"
      },
      "source": [
        "### 3.5.1 Training GMF and MLP model\n",
        "`model.save`, we can set the `dir_name` to store the parameters of GMF and MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ni3QlIwRcuT3",
        "outputId": "ac535599-d5e3-4eb5-bb91-f761fea4a668"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        }
      ],
      "source": [
        "model = NCF (\n",
        "    n_users=data.n_users, \n",
        "    n_items=data.n_items,\n",
        "    model_type=\"GMF\",\n",
        "    n_factors=4,\n",
        "    layer_sizes=[16,8,4],\n",
        "    n_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=1e-3,\n",
        "    verbose=10,\n",
        "    seed=SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lulAQrZBcuT3",
        "outputId": "85e94919-bc7b-4011-cc39-7bd9568f0dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Took 728.4809228709998 seconds for training.\n"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    model.fit(data)\n",
        "\n",
        "print(\"Took {} seconds for training.\".format(train_time.interval))\n",
        "\n",
        "model.save(dir_name=\".pretrain/GMF\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WII4I1IGcuT3"
      },
      "outputs": [],
      "source": [
        "model = NCF (\n",
        "    n_users=data.n_users, \n",
        "    n_items=data.n_items,\n",
        "    model_type=\"MLP\",\n",
        "    n_factors=4,\n",
        "    layer_sizes=[16,8,4],\n",
        "    n_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=1e-3,\n",
        "    verbose=10,\n",
        "    seed=SEED\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LijLBfngcuT3",
        "outputId": "532ae195-e0e4-4f67-8e85-b6d5dd2f26c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Took 767.5178999250002 seconds for training.\n"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    model.fit(data)\n",
        "\n",
        "print(\"Took {} seconds for training.\".format(train_time.interval))\n",
        "\n",
        "model.save(dir_name=\".pretrain/MLP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NzgDtkjcuT4"
      },
      "source": [
        "### 3.5.2 Load pre-trained GMF and MLP model for NeuMF\n",
        "`model.load`, we can set the `gmf_dir` and `mlp_dir` to store the parameters for NeuMF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nKfiBhXUcuT4"
      },
      "outputs": [],
      "source": [
        "model = NCF (\n",
        "    n_users=data.n_users, \n",
        "    n_items=data.n_items,\n",
        "    model_type=\"NeuMF\",\n",
        "    n_factors=4,\n",
        "    layer_sizes=[16,8,4],\n",
        "    n_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=1e-3,\n",
        "    verbose=10,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "model.load(gmf_dir=\".pretrain/GMF\", mlp_dir=\".pretrain/MLP\", alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iidA1YGLcuT4",
        "outputId": "811c3a17-699f-4131-dfa3-01d46b12e3fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Took 803.6544212169997 seconds for training.\n"
          ]
        }
      ],
      "source": [
        "with Timer() as train_time:\n",
        "    model.fit(data)\n",
        "\n",
        "print(\"Took {} seconds for training.\".format(train_time.interval))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm8K13BYcuT4"
      },
      "source": [
        "### 3.5.3 Compare with not pre-trained NeuMF\n",
        "\n",
        "You can use beforementioned evaluation methods to evaluate the pre-trained `NCF` Model. Usually, we will find the performance of pre-trained NCF is better than the not pre-trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vT0twgDUcuT4",
        "outputId": "62d7ea90-e62b-4f71-be85-119d33df1a2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Took 4.398782714999925 seconds for prediction.\n"
          ]
        }
      ],
      "source": [
        "with Timer() as test_time:\n",
        "\n",
        "    users, items, preds = [], [], []\n",
        "    item = list(train.itemID.unique())\n",
        "    for user in train.userID.unique():\n",
        "        user = [user] * len(item) \n",
        "        users.extend(user)\n",
        "        items.extend(item)\n",
        "        preds.extend(list(model.predict(user, item, is_list=True)))\n",
        "\n",
        "    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
        "\n",
        "    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
        "    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
        "\n",
        "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y0LCL-zwcuT5",
        "outputId": "2a620af6-f472-4a11-e7b9-a1e3443d598e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAP:\t0.044157\n",
            "NDCG:\t0.183570\n",
            "Precision@K:\t0.164899\n",
            "Recall@K:\t0.093827\n"
          ]
        }
      ],
      "source": [
        "eval_map2 = map_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "eval_ndcg2 = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "eval_precision2 = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "eval_recall2 = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
        "\n",
        "print(\"MAP:\\t%f\" % eval_map2,\n",
        "      \"NDCG:\\t%f\" % eval_ndcg2,\n",
        "      \"Precision@K:\\t%f\" % eval_precision2,\n",
        "      \"Recall@K:\\t%f\" % eval_recall2, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-aqaDoXFcuT5",
        "outputId": "ffc113b4-f692-44da-8ad1-9b83363d4aae"
      },
      "outputs": [
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": 0.04815803171963362,
              "encoder": "json",
              "name": "map",
              "version": 1
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": 0.39977299553939927,
              "encoder": "json",
              "name": "ndcg",
              "version": 1
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": 0.17995758218451752,
              "encoder": "json",
              "name": "precision",
              "version": 1
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": 0.1006277028281861,
              "encoder": "json",
              "name": "recall",
              "version": 1
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": 0.04415668514437189,
              "encoder": "json",
              "name": "map2",
              "version": 1
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": 0.183570372729029,
              "encoder": "json",
              "name": "ndcg2",
              "version": 1
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": 0.16489925768822905,
              "encoder": "json",
              "name": "precision2",
              "version": 1
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": 0.09382736339298675,
              "encoder": "json",
              "name": "recall2",
              "version": 1
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Record results with papermill for tests\n",
        "sb.glue(\"map\", eval_map)\n",
        "sb.glue(\"ndcg\", eval_ndcg)\n",
        "sb.glue(\"precision\", eval_precision)\n",
        "sb.glue(\"recall\", eval_recall)\n",
        "sb.glue(\"map2\", eval_map2)\n",
        "sb.glue(\"ndcg2\", eval_ndcg2)\n",
        "sb.glue(\"precision2\", eval_precision2)\n",
        "sb.glue(\"recall2\", eval_recall2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdrvr5oGcuT5"
      },
      "source": [
        "### 3.5.4 Delete pre-trained directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1B2DtG-tcuT5",
        "outputId": "bca91632-c8e8-4717-9ec5-8ad372c0cd64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Did '.pretrain' exist?: False\n"
          ]
        }
      ],
      "source": [
        "save_dir = \".pretrain\"\n",
        "if os.path.exists(save_dir):\n",
        "    shutil.rmtree(save_dir)\n",
        "    \n",
        "print(\"Did \\'%s\\' exist?: %s\" % (save_dir, os.path.exists(save_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrVouPJScuT6"
      },
      "source": [
        "### Reference: \n",
        "1. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu & Tat-Seng Chua, Neural Collaborative Filtering, 2017, https://arxiv.org/abs/1708.05031\n",
        "\n",
        "2. Official NCF implementation [Keras with Theano]: https://github.com/hexiangnan/neural_collaborative_filtering\n",
        "\n",
        "3. Other nice NCF implementation [Pytorch]: https://github.com/LaceyChen17/neural-collaborative-filtering\n",
        "\n",
        "jiperez11@uc.cl"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Tags",
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "3a9a0c422ff9f08d62211b9648017c63b0a26d2c935edc37ebb8453675d13bb5"
    },
    "kernelspec": {
      "display_name": "reco_gpu",
      "language": "python",
      "name": "conda-env-reco_gpu-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}